{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np \n",
    "from more_itertools import pairwise\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(a) Warmup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the 10 most frequent words in each language.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent words in swedish is: [('att', 9181), ('och', 7038), ('i', 5949), ('det', 5687), ('som', 5028), ('för', 4959), ('av', 4013), ('är', 3840), ('en', 3724), ('vi', 3211)]\n",
      "The 10 most frequent words in english is: [('the', 19322), ('of', 9312), ('to', 8801), ('and', 6946), ('in', 6090), ('is', 4400), ('that', 4357), ('a', 4269), ('we', 3223), ('this', 3222)]\n"
     ]
    }
   ],
   "source": [
    "with open('dat410_europarl/europarl-v7.sv-en.lc.en') as f:\n",
    "    eng = f.readlines()\n",
    "\n",
    "with open('dat410_europarl/europarl-v7.sv-en.lc.sv') as f:\n",
    "    swe = f.readlines()\n",
    "\n",
    "swe_sentences = []\n",
    "eng_sentences = []\n",
    "\n",
    "for str in swe:\n",
    "        swe_sentences.append(str.split(' '))\n",
    "\n",
    "for str in eng:\n",
    "        eng_sentences.append(str.split(' '))\n",
    "\n",
    "swe_words = [item for sublist in swe_sentences for item in sublist]\n",
    "eng_words = [item for sublist in eng_sentences for item in sublist]\n",
    "\n",
    "swe_words = [i for i in swe_words if i.isalpha()]\n",
    "eng_words = [i for i in eng_words if i.isalpha()]\n",
    "\n",
    "n = 10\n",
    "\n",
    "counted_swe = Counter(swe_words).most_common(n)\n",
    "counted_eng = Counter(eng_words).most_common(n)\n",
    "\n",
    "print(f'The {n} most frequent words in swedish is: {counted_swe}')\n",
    "print(f'The {n} most frequent words in english is: {counted_eng}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's assume that we pick a word completely randomly from the European parliament proceedings. According to your estimate, what is the probability that it is speaker? What is the probability that it is zebra?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the word is zebra is 0.0%, as zebra is not a word in our data set.\n",
      "The probability that the word is speaker is around 0.00399%.\n"
     ]
    }
   ],
   "source": [
    "n_zebra = eng_words.count('zebra')\n",
    "prob_zebra = n_zebra/len(eng_words)\n",
    "print(f'The probability that the word is zebra is {prob_zebra*100}%, as zebra is not a word in our data set.')\n",
    "\n",
    "\n",
    "n_speaker = eng_words.count('speaker')\n",
    "prob_speaker = n_speaker/len(eng_words)\n",
    "print(f'The probability that the word is speaker is around {round(prob_speaker*100, 5)}%.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(b) Language modeling**\n",
    "**Implement a bigram language model as described in the lecture, and use it to compute the probability of a short sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel():\n",
    "\n",
    "    def __init__(self, word_list, word_pair_list):\n",
    "        self.word_list = word_list\n",
    "        self.word_pair_list = word_pair_list\n",
    "\n",
    "    # Calculate probability of a sentence\n",
    "    def predict(self, sentence):\n",
    "        words = sentence.split()\n",
    "        prob = self.prob_of_word(words[0])\n",
    "    \n",
    "        for i in range(0, len(words) - 1):\n",
    "            w1 = words[i]\n",
    "            w2 = words[i+1]\n",
    "    \n",
    "            prob = prob * self.count_of_word_pair(w1, w2)/self.count_of_word(w1)\n",
    "    \n",
    "        return prob\n",
    "    \n",
    "    def count_of_word(self, word):\n",
    "        return self.word_list.count(word)\n",
    "    \n",
    "    def count_of_word_pair(self, word1, word2):\n",
    "        return self.word_pair_list.count((word1, word2))\n",
    "    \n",
    "    # Calculate probability of a word appearing\n",
    "    # all_words should be a list \n",
    "    # ['hello', 'my', 'name', 'is', 'lisa']\n",
    "    def prob_of_word(self, word):\n",
    "        n_word = self.word_list.count(word)\n",
    "        n_words = len(self.word_list)\n",
    "        prob = n_word/n_words\n",
    "        return prob\n",
    "\n",
    "# data should be list of lists with elements that are words\n",
    "# [['hello', 'my', 'name', 'is', 'lisa'], ['hej', 'jag', 'gillar', 'att', 'äta']]\n",
    "def get_word_pairs(data):\n",
    "    sentence_pairs = []\n",
    "    for d in data:\n",
    "        sentence_pairs.append(list(pairwise(d)))\n",
    "    \n",
    "    word_pairs = [item for sublist in sentence_pairs for item in sublist]\n",
    "    return word_pairs\n",
    "\n",
    "# lst should be list of lists with elements that are words\n",
    "# [['hello', 'my', 'name', 'is', 'lisa'], ['hej', 'jag', 'gillar', 'att', 'äta']]\n",
    "def clean_data(lst):\n",
    "    new_list = []\n",
    "    for sentence in lst:\n",
    "        sentence_list = []\n",
    "        for word in sentence:\n",
    "            if word.isalpha():\n",
    "                sentence_list.append(word)\n",
    "\n",
    "        new_list.append(sentence_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the sentence 'in the meeting' is 1.7272e-06\n"
     ]
    }
   ],
   "source": [
    "with open('dat410_europarl/europarl-v7.sv-en.lc.en') as f:\n",
    "    eng = f.read()\n",
    "\n",
    "eng_lst = eng.split('.')\n",
    "\n",
    "list_of_sentences = [sentence.split() for sentence in eng_lst]\n",
    "list_of_sentences = clean_data(list_of_sentences)\n",
    "\n",
    "word_pairs = get_word_pairs(list_of_sentences)     \n",
    "\n",
    "test_sentence = 'in the meeting'\n",
    "\n",
    "model = BigramModel(eng_words, word_pairs)\n",
    "prob = model.predict(test_sentence)\n",
    "print(f'The probability of the sentence \\'{test_sentence}\\' is {round(prob, 10)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? And what happens if your sentence is very long (e.g. 100 words or more)?** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to compute the probability of a sentence that contains a word that doesn't appear in the training texts, we will get an error. This is because we will perform division by zero, as this word will have a count of 0. If our sentence is very long, the probability will converge to zero. It is likely that we will eventually get a word pair that doesn't exist in the training texts, and then we multiply by 0. Even if all word pairs exists the probability will of course be very low if the sentence is long."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c) Translation modeling**\n",
    "**Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dat410_europarl/europarl-v7.sv-en.lc.en') as f:\n",
    "    eng = f.readlines()\n",
    "\n",
    "with open('dat410_europarl/europarl-v7.sv-en.lc.sv') as f:\n",
    "    swe = f.readlines()\n",
    "\n",
    "# Clean the data\n",
    "eng_clean = []\n",
    "for word in eng: \n",
    "    eng_clean.append(word.replace(',', \"\").replace('.', \"\").replace('\\n', \"\"))\n",
    "eng = eng_clean\n",
    "\n",
    "# Clean the data\n",
    "swe_clean = []\n",
    "for word in swe: \n",
    "    swe_clean.append(word.replace(',', \"\").replace('.', \"\").replace('\\n', \"\"))\n",
    "swe = swe_clean\n",
    "\n",
    "all_eng_words = eng_words\n",
    "all_swe_words = swe_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words_to_ignore = 25\n",
    "\n",
    "# Get unique elements and their counts\n",
    "swe_unique, swe_counts = np.unique(all_swe_words, return_counts=True)\n",
    "\n",
    "# Only keep words that appear more than 25 times\n",
    "all_swe_words = swe_unique[swe_counts > n_words_to_ignore]\n",
    "\n",
    "# Get unique elements and their counts\n",
    "eng_unique, eng_counts = np.unique(all_eng_words, return_counts=True)\n",
    "\n",
    "# Only keep words that appear more than 25 times\n",
    "all_eng_words = eng_unique[eng_counts > n_words_to_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM iterations\n",
    "n_iterations = 30\n",
    "t_init = 0.0001\n",
    "\n",
    "# Initialize t \n",
    "t = defaultdict(lambda: t_init)   \n",
    "\n",
    "for i in range(n_iterations):\n",
    "    count_swe_eng = defaultdict(float)\n",
    "    count_eng = defaultdict(float)\n",
    "\n",
    "    # for each sentence pair\n",
    "    for k in range(len(eng)):\n",
    "        swe_words_in_sentence = np.array(swe[k].split())\n",
    "        eng_words_in_sentence = np.array(eng[k].split())\n",
    "       \n",
    "        eng_words_in_sentence = np.append(eng_words_in_sentence, 'NULL')\n",
    "\n",
    "        for swe_word in swe_words_in_sentence: \n",
    "        \n",
    "            # compute alignment prob\n",
    "            lst_t = np.array([t[(swe_word, word)] for word in eng_words_in_sentence])\n",
    "            sum_t = np.sum(lst_t)\n",
    "\n",
    "            for eng_word in eng_words_in_sentence:\n",
    "                alignment_prob = t[(swe_word, eng_word)]/sum_t\n",
    "\n",
    "                # update pseudocount\n",
    "                count_swe_eng[(eng_word, swe_word)] += alignment_prob\n",
    "\n",
    "                # update pseudocount\n",
    "                count_eng[eng_word] += alignment_prob   \n",
    "\n",
    "                # reestimate probabilities\n",
    "                t[(swe_word, eng_word)] = count_swe_eng[(eng_word, swe_word)]/count_eng[eng_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten Swedish words it is most probable that \"european\" is translated to are: ['europeiska', 'den', 'på', 'europaparlamentets', 'efter', 'jag', 'särskilda', 'israel', 'alldeles', 'ytterst']\n"
     ]
    }
   ],
   "source": [
    "# Getting the probabilitie for each word pair\n",
    "keys = []\n",
    "probs = []\n",
    "for word in set(all_swe_words):\n",
    "    keys.append(word)\n",
    "    probs.append(t[(word, 'european')])\n",
    "    \n",
    "n = 10\n",
    "\n",
    "# getting the n most probable translations\n",
    "n_most_probable = sorted(zip(keys, probs), key=lambda x: x[1], reverse=True)[0:n]\n",
    "print(f'The ten Swedish words it is most probable that \"european\" is translated to are: {[i[0] for i in n_most_probable]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(d) Decoding**\n",
    "**Define and implement an algorithm to find a translation, given a sentence in the source language.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can we talk'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_probable(eng_words, target):\n",
    "    keys = []\n",
    "    probs = []\n",
    "    for word in set(eng_words):\n",
    "        keys.append(word)\n",
    "        probs.append(t[(target, word)])\n",
    "\n",
    "    most_probable = sorted(zip(keys, probs), key=lambda x: x[1], reverse=True)[0:10]\n",
    "    return most_probable[0][0]\n",
    "\n",
    "def get_translation(sentence):\n",
    "\n",
    "    translation = ''\n",
    "    for word in sentence.split(): \n",
    "        translation += get_most_probable(all_eng_words, word) + ' '\n",
    "\n",
    "    return translation\n",
    "\n",
    "def get_permutations(sentence):\n",
    "    words = sentence.split()\n",
    "    return itertools.permutations(words, len(words))\n",
    "\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    translation = get_translation(sentence)\n",
    "\n",
    "    permutations = get_permutations(translation)\n",
    "    permutations = list(permutations)\n",
    "    max_prob = 0\n",
    "    most_probable_translation = []\n",
    "\n",
    "    for perm_sentence in permutations:\n",
    "        perm_sentence = ' '.join(perm_sentence)\n",
    "        prob = model.predict(perm_sentence)\n",
    "        \n",
    "        if prob > max_prob:\n",
    "            most_probable_translation = perm_sentence\n",
    "            max_prob = prob\n",
    "        \n",
    "    if max_prob == 0:\n",
    "        print('No probable translation found, translation word by word is:')\n",
    "        return translation\n",
    "    \n",
    "    translation = ''.join(most_probable_translation) \n",
    "    \n",
    "    return translation\n",
    "\n",
    "sentence = 'ni är från europa'\n",
    "sentence = 'vi kan tala'\n",
    "\n",
    "translate_sentence(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e2c82196e12baaed2ed09f9a042bc281a80ecc12c17ec56f626fcbf6ffbb0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
