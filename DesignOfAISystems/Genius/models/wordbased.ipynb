{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Look, I was gonna go easy on you not to hurt your feelings\"\n",
      "\"But I'm only going to get this one chance\" (Six minutes—, six minutes—)\n",
      "\"Something's wrong, I can feel it\" (Six minutes, Slim Shady, you'r\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/eminem.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'i', 'was', 'gonna', 'go', 'easy', 'on', 'you', 'not', 'to', 'hurt', 'your', 'feelings', 'but', 'im', 'only', 'going', 'to', 'get', 'this', 'one', 'chance', 'six', 'six', 'somethings', 'wrong', 'i', 'can', 'feel', 'it', 'six', 'minutes', 'slim', 'shady', 'youre', 'on', 'just', 'a', 'feeling', 'ive', 'got', 'like', 'somethings', 'about', 'to', 'happen', 'but', 'i', 'dont', 'know', 'what', 'if', 'that', 'means', 'what', 'i', 'think', 'it', 'means', 'were', 'in', 'trouble', 'big', 'trouble', 'and', 'if', 'he', 'is', 'as', 'bananas', 'as', 'you', 'say', 'im', 'not', 'taking', 'any', 'chances', 'you', 'are', 'just', 'what', 'the', 'doc', 'ordered', 'im', 'beginnin', 'to', 'feel', 'like', 'a', 'rap', 'god', 'rap', 'god', 'all', 'my', 'people', 'from', 'the', 'front', 'to', 'the', 'back', 'nod', 'back', 'nod', 'now', 'who', 'thinks', 'their', 'arms', 'are', 'long', 'enough', 'to', 'slap', 'box', 'slap', 'box', 'they', 'said', 'i', 'rap', 'like', 'a', 'robot', 'so', 'call', 'me', 'rapbot', 'but', 'for', 'me', 'to', 'rap', 'like', 'a', 'computer', 'it', 'must', 'be', 'in', 'my', 'genes', 'i', 'got', 'a', 'laptop', 'in', 'my', 'back', 'pocket', 'my', 'penll', 'go', 'off', 'when', 'i', 'halfcock', 'it', 'got', 'a', 'fat', 'knot', 'from', 'that', 'rap', 'profit', 'made', 'a', 'livin', 'and', 'a', 'killin', 'off', 'it', 'ever', 'since', 'bill', 'clinton', 'was', 'still', 'in', 'office', 'with', 'monica', 'lewinsky', 'feelin', 'on', 'his', 'nutsack', 'im', 'an', 'mc', 'still', 'as', 'honest', 'but', 'as']\n",
      "Total Tokens: 31995\n",
      "Unique Tokens: 2305\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 31944\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    " # select sequence of tokens\n",
    " seq = tokens[i-length:i]\n",
    " # convert into a line\n",
    " line = ' '.join(seq)\n",
    " # store\n",
    " sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    " data = '\\n'.join(lines)\n",
    " file = open(filename, 'w')\n",
    " file.write(data)\n",
    " file.close()\n",
    "\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    " # open the file as read only\n",
    " file = open(filename, 'r')\n",
    " # read all text\n",
    " text = file.read()\n",
    " # close the file\n",
    " file.close()\n",
    " return text\n",
    " \n",
    "# load\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:01:23.250941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# integer encode sequences of words\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:01:27.640819: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            115300    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2306)              232906    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 499,106\n",
      "Trainable params: 499,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(tf.keras.layers.LSTM(100, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 26s 94ms/step - loss: 6.4370 - accuracy: 0.0328\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 5.9939 - accuracy: 0.0372\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 5.7487 - accuracy: 0.0401\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 5.5569 - accuracy: 0.0486\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 5.3480 - accuracy: 0.0546\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 5.1380 - accuracy: 0.0648\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 5.0142 - accuracy: 0.0732\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 4.8009 - accuracy: 0.0828\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 26s 106ms/step - loss: 4.6283 - accuracy: 0.0998\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 4.3837 - accuracy: 0.1199\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 4.1535 - accuracy: 0.1381\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 3.9458 - accuracy: 0.1598\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 3.7496 - accuracy: 0.1821\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 3.5617 - accuracy: 0.2086\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 3.3800 - accuracy: 0.2334\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 3.2215 - accuracy: 0.2601\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 3.0990 - accuracy: 0.2832\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 3.0958 - accuracy: 0.2930\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 2.9165 - accuracy: 0.3169\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 2.7105 - accuracy: 0.3540\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 26s 106ms/step - loss: 2.6056 - accuracy: 0.3777\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 2.4876 - accuracy: 0.4044\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 2.3613 - accuracy: 0.4304\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 2.2083 - accuracy: 0.4619\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 2.1050 - accuracy: 0.4840\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 2.0127 - accuracy: 0.5078\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.9899 - accuracy: 0.5178\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 1.8521 - accuracy: 0.5475\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 1.7379 - accuracy: 0.5743\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 1.6879 - accuracy: 0.5850\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 26s 106ms/step - loss: 1.6182 - accuracy: 0.6013\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 1.4893 - accuracy: 0.6293\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.5298 - accuracy: 0.6328\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.4019 - accuracy: 0.6571\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 1.3230 - accuracy: 0.6814\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 1.2817 - accuracy: 0.6909\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 1.1601 - accuracy: 0.7147\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.1227 - accuracy: 0.7255\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.1215 - accuracy: 0.7269\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.3320 - accuracy: 0.6931\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 1.2601 - accuracy: 0.6996\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.1753 - accuracy: 0.7200\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 1.0719 - accuracy: 0.7433\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 1.0058 - accuracy: 0.7625\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 1.0377 - accuracy: 0.7563\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.9070 - accuracy: 0.7831\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.9337 - accuracy: 0.7814\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 1.1542 - accuracy: 0.7461\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 2.4427 - accuracy: 0.5206\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.8443 - accuracy: 0.5817\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 1.6258 - accuracy: 0.6191\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.4845 - accuracy: 0.6452\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.3741 - accuracy: 0.6665\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 1.2747 - accuracy: 0.6910\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.2017 - accuracy: 0.7077\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.1242 - accuracy: 0.7249\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 1.0549 - accuracy: 0.7436\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.0012 - accuracy: 0.7571\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.9324 - accuracy: 0.7730\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.8914 - accuracy: 0.7829\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.8541 - accuracy: 0.7930\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.7877 - accuracy: 0.8073\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.7355 - accuracy: 0.8250\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.6955 - accuracy: 0.8318\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.6547 - accuracy: 0.8420\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.6133 - accuracy: 0.8559\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.5778 - accuracy: 0.8649\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.5448 - accuracy: 0.8720\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 26s 106ms/step - loss: 0.5117 - accuracy: 0.8834\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.4902 - accuracy: 0.8860\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.4499 - accuracy: 0.8970\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.4240 - accuracy: 0.9050\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3878 - accuracy: 0.9169\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.3519 - accuracy: 0.9264\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3405 - accuracy: 0.9282\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.3499 - accuracy: 0.9234\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3132 - accuracy: 0.9354\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.2609 - accuracy: 0.9515\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2398 - accuracy: 0.9564\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.2264 - accuracy: 0.9597\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2225 - accuracy: 0.9601\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.2686 - accuracy: 0.9431\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.2802 - accuracy: 0.9357\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.1972 - accuracy: 0.9648\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.1398 - accuracy: 0.9802\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.1366 - accuracy: 0.9818\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1687 - accuracy: 0.9711\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2253 - accuracy: 0.9518\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.1550 - accuracy: 0.9738\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.0956 - accuracy: 0.9890\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.0773 - accuracy: 0.9922\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.0851 - accuracy: 0.9899\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2840 - accuracy: 0.9241\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.1709 - accuracy: 0.9636\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.0698 - accuracy: 0.9935\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.0511 - accuracy: 0.9961\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.0448 - accuracy: 0.9970\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.0411 - accuracy: 0.9972\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.0418 - accuracy: 0.9972\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 0.0596 - accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faea9f83e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    " # open the file as read only\n",
    " file = open(filename, 'r')\n",
    " # read all text\n",
    " text = file.read()\n",
    " # close the file\n",
    " file.close()\n",
    " return text\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and thats the message that we deliver to little kids and expect them not to know what a womans clitoris is of course theyre gonna know what intercourse is by the time they hit fourth grade theyve got the discovery channel dont they we aint nothin but some of us cannibals\n",
      "\n",
      "1/1 [==============================] - 1s 557ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "who cut partner but the beat meets really all on a broken motherfucker or wanna turn a pic and if hard empty without me so come on and dip bum on your lips and if you feel like i feel im but theyre gonna get it but im means i\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\t#yhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\tpredict_y=model.predict(encoded) \n",
    "\t\tyhat=np.argmax(predict_y,axis=1)\n",
    "\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
